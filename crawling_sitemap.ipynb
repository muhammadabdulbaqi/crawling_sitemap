{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/6JmjUge6lLNp/ifryyiM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhammadabdulbaqi/crawling_sitemap/blob/main/crawling_sitemap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 requests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vwi5LrWc_t5",
        "outputId": "2915c811-6b2b-4c4f-9bf3-57aa41af848b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kfvPt4nxcyGH"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from datetime import datetime\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Function to get all links on a page within the specified path\n",
        "def get_links(url, base_url, allowed_path, visited):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        links = set()\n",
        "\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            href = a_tag['href']\n",
        "            absolute_url = urljoin(base_url, href)  # Convert relative URLs to absolute\n",
        "            parsed_url = urlparse(absolute_url)\n",
        "\n",
        "            # Include only internal links within the allowed path\n",
        "            if parsed_url.netloc == urlparse(base_url).netloc and absolute_url.startswith(allowed_path):\n",
        "                links.add(absolute_url)\n",
        "\n",
        "        return links\n",
        "    except Exception as e:\n",
        "        print(f\"Error while crawling {url}: {e}\")\n",
        "        return set()\n",
        "\n",
        "# Sitemap generator function\n",
        "def generate_sitemap(base_url, allowed_path):\n",
        "    to_visit = {allowed_path}\n",
        "    visited = set()\n",
        "    all_links = set()\n",
        "\n",
        "    while to_visit:\n",
        "        current_url = to_visit.pop()\n",
        "        if current_url not in visited:\n",
        "            #print(f\"Crawling: {current_url}\")\n",
        "            visited.add(current_url)\n",
        "            links = get_links(current_url, base_url, allowed_path, visited)\n",
        "            to_visit.update(links - visited)\n",
        "            all_links.update(links)\n",
        "\n",
        "    return all_links\n",
        "\n",
        "# Write sitemap to XML file\n",
        "def write_sitemap_to_xml(links, output_file=\"sitemap.xml\"):\n",
        "    now = datetime.now().isoformat()\n",
        "    xml_content = '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n'\n",
        "    xml_content += '<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\\n'\n",
        "\n",
        "    for link in sorted(links):\n",
        "        xml_content += \"  <url>\\n\"\n",
        "        xml_content += f\"    <loc>{link}</loc>\\n\"\n",
        "        xml_content += f\"    <lastmod>{now}</lastmod>\\n\"\n",
        "        xml_content += \"    <changefreq>daily</changefreq>\\n\"\n",
        "        xml_content += \"    <priority>0.8</priority>\\n\"\n",
        "        xml_content += \"  </url>\\n\"\n",
        "\n",
        "    xml_content += \"</urlset>\\n\"\n",
        "\n",
        "    with open(output_file, \"w\") as file:\n",
        "        file.write(xml_content)\n",
        "    print(f\"Sitemap saved to {output_file}\")\n",
        "\n",
        "# Function to extract URLs from XML content\n",
        "def extract_urls_from_xml(file_path):\n",
        "    try:\n",
        "        with open(file_path, \"r\") as file:\n",
        "            xml_content = file.read()\n",
        "        # Parse the XML content\n",
        "        root = ET.fromstring(xml_content)\n",
        "        # Extract URLs from <loc> tags\n",
        "        urls = [loc.text for loc in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\")]\n",
        "        return urls\n",
        "    except ET.ParseError as e:\n",
        "        print(f\"XML parsing error: {e}\")\n",
        "        return []\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return []\n",
        "\n",
        "# Main\n",
        "if __name__ == \"__main__\":\n",
        "    # Base URL and allowed path\n",
        "    base_url = \"https://www.enterprisedb.com/\"\n",
        "    allowed_path = \"https://www.enterprisedb.com/docs/epas/11/\"\n",
        "\n",
        "    # Generate sitemap\n",
        "    sitemap_links = generate_sitemap(base_url, allowed_path)\n",
        "    sitemap_file = \"sitemap.xml\"\n",
        "    write_sitemap_to_xml(sitemap_links, sitemap_file)\n",
        "\n",
        "    # Extract URLs from the generated sitemap and save them to a text file\n",
        "    urls = extract_urls_from_xml(sitemap_file)\n",
        "    if urls:\n",
        "        output_file = \"urls.txt\"\n",
        "        with open(output_file, \"w\") as f:\n",
        "            for url in urls:\n",
        "                f.write(url + \"\\n\")\n",
        "        print(f\"Extracted URLs saved to {output_file}\")\n",
        "    else:\n",
        "        print(\"No URLs found or XML is invalid.\")\n"
      ]
    }
  ]
}